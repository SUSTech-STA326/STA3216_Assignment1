{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA326 Assignment 1: Data Collection\n",
    "This is an assignment that is openly available for the Data Science Practice (STA326). \n",
    "\n",
    "The assignment encapsulates a holistic approach towards data collection and analysis, covering a spectrum of data formats and sources. Our objective is to amass, process, and scrutinize data to unearth significant insights. The methodology is sectioned into four pivotal tasks: \n",
    "- Web scraping \n",
    "- JSON file analysis \n",
    "- Working with CSV files\n",
    "- Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests  # send request\n",
    "from bs4 import BeautifulSoup  # parse web pages\n",
    "import pandas as pd  # csv\n",
    "from time import sleep  # wait\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web Scraping\n",
    "\n",
    "In this assignment, we will explore web scraping, which can often include diverse information from website, and also use the data for simple analysis. We take [douban](https://movie.douban.com/top250) as the target website in this assignment.\n",
    "\n",
    "### Scraping Rules\n",
    "\n",
    "1) If you are using another organization's website for scraping, make sure to check the website's terms & conditions. \n",
    "\n",
    "2) Do not request data from the website too aggressively (quickly) with your program (also known as spamming), as this may break the website. Make sure your program behaves in a reasonable manner (i.e. acts like a human). One request for one webpage per second is good practice.\n",
    "\n",
    "3) The layout of a website may change from time to time. Because of this, if you're scraping a website, make sure to revisit the site and rewrite your code as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Web Scrape\n",
    "\n",
    "In order to extract the data we want, we’ll start with extracting the whole web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a request header (to prevent anti-scraping)\n",
    "headers = {\n",
    "    'authority': 'curlconverter.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'if-modified-since': 'Fri, 15 Jul 2022 21:44:42 GMT',\n",
    "    'if-none-match': 'W/\"62d1dfca-3a58\"',\n",
    "    'referer': 'https://curlconverter.com/',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Microsoft Edge\";v=\"102\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Linux\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'cross-site',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process can be split into three steps:\n",
    "\n",
    "1. Make a variable called `url`, that stores the following URL (as a string):\n",
    "https://movie.douban.com/top250?start=0\n",
    "\n",
    "2. Now, to open the URL, use `requests.get()` and provide `url` and `headers` as its input. Store this in a variable called `page`.\n",
    "\n",
    "3. After that, make a variable called `soup` to parse the HTML using `BeautifulSoup`. Consider that there will be a method from `BeautifulSoup` that you'll need to call on to get the content from the page. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "url='https://movie.douban.com/top250?start=0'\n",
    "page = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert url\n",
    "assert page\n",
    "assert soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Data Extraction\n",
    "\n",
    "Extract the data (`name` and `star`) from the page and save it in the corresponding list like `movie_name` and `movie_star`. \n",
    "\n",
    "Make sure you extract it as a string.\n",
    "\n",
    "To do so, you have to use the soup object created in the above cell.\n",
    "\n",
    "**Hint**: from your soup variable, you can access this with `soup.select()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['肖申克的救赎\\xa0/\\xa0The Shawshank Redemption\\xa0/\\xa0月黑高飞(港)  /  刺激1995(台)', '霸王别姬\\xa0/\\xa0再见，我的妾  /  Farewell My Concubine', '阿甘正传\\xa0/\\xa0Forrest Gump\\xa0/\\xa0福雷斯特·冈普', '泰坦尼克号\\xa0/\\xa0Titanic\\xa0/\\xa0铁达尼号(港 / 台)', '千与千寻\\xa0/\\xa0千と千尋の神隠し\\xa0/\\xa0神隐少女(台)  /  千与千寻的神隐', '这个杀手不太冷\\xa0/\\xa0Léon\\xa0/\\xa0终极追杀令(台)  /  杀手莱昂', '美丽人生\\xa0/\\xa0La vita è bella\\xa0/\\xa0一个快乐的传说(港)  /  Life Is Beautiful', '星际穿越\\xa0/\\xa0Interstellar\\xa0/\\xa0星际启示录(港)  /  星际效应(台)', '盗梦空间\\xa0/\\xa0Inception\\xa0/\\xa0潜行凶间(港)  /  全面启动(台)', \"辛德勒的名单\\xa0/\\xa0Schindler's List\\xa0/\\xa0舒特拉的名单(港)  /  辛德勒名单\", '楚门的世界\\xa0/\\xa0The Truman Show\\xa0/\\xa0真人Show(港)  /  真人戏', \"忠犬八公的故事\\xa0/\\xa0Hachi: A Dog's Tale\\xa0/\\xa0秋田犬八千(港)  /  忠犬小八(台)\", \"海上钢琴师\\xa0/\\xa0La leggenda del pianista sull'oceano\\xa0/\\xa0声光伴我飞(港)  /  一九零零的传奇\", '三傻大闹宝莱坞\\xa0/\\xa03 Idiots\\xa0/\\xa0三个傻瓜(台)  /  作死不离3兄弟(港)', '放牛班的春天\\xa0/\\xa0Les choristes\\xa0/\\xa0歌声伴我心(港)  /  唱诗班男孩', '机器人总动员\\xa0/\\xa0WALL·E\\xa0/\\xa0太空奇兵·威E(港)  /  瓦力(台)', '疯狂动物城\\xa0/\\xa0Zootopia\\xa0/\\xa0优兽大都会(港)  /  动物方城市(台)', '无间道\\xa0/\\xa0無間道\\xa0/\\xa0Infernal Affairs  /  Mou gaan dou', '控方证人\\xa0/\\xa0Witness for the Prosecution\\xa0/\\xa0雄才伟略  /  情妇', '大话西游之大圣娶亲\\xa0/\\xa0西遊記大結局之仙履奇緣\\xa0/\\xa0西游记完结篇仙履奇缘  /  齐天大圣西游记', '熔炉\\xa0/\\xa0도가니\\xa0/\\xa0无声呐喊(港)  /  漩涡', \"教父\\xa0/\\xa0The Godfather\\xa0/\\xa0Mario Puzo's The Godfather\", '触不可及\\xa0/\\xa0Intouchables\\xa0/\\xa0闪亮人生(港)  /  逆转人生(台)', '当幸福来敲门\\xa0/\\xa0The Pursuit of Happyness\\xa0/\\xa0寻找快乐的故事(港)  /  追求快乐', '寻梦环游记\\xa0/\\xa0Coco\\xa0/\\xa0玩转极乐园(港)  /  可可夜总会(台)', \"末代皇帝\\xa0/\\xa0The Last Emperor\\xa0/\\xa0末代皇帝溥仪(港)  /  L'ultimo imperatore\", '龙猫\\xa0/\\xa0となりのトトロ\\xa0/\\xa0邻居托托罗  /  邻家的豆豆龙', '怦然心动\\xa0/\\xa0Flipped\\xa0/\\xa0萌动青春  /  青春萌动', '活着\\xa0/\\xa0人生  /  Lifetimes', \"哈利·波特与魔法石\\xa0/\\xa0Harry Potter and the Sorcerer's Stone\\xa0/\\xa0哈利波特1：神秘的魔法石(港 / 台) \", '蝙蝠侠：黑暗骑士\\xa0/\\xa0The Dark Knight\\xa0/\\xa0蝙蝠侠前传2：黑暗骑士  /  黑暗骑士(台)', '指环王3：王者无敌\\xa0/\\xa0The Lord of the Rings: The Return of the King\\xa0/\\xa0魔戒三部曲：王者再临(台 / 港) ', '我不是药神\\xa0/\\xa0中国药神  /  印度药神', '乱世佳人\\xa0/\\xa0Gone with the Wind\\xa0/\\xa0飘', '飞屋环游记\\xa0/\\xa0Up\\xa0/\\xa0冲天救兵(港)  /  天外奇迹(台)', '素媛\\xa0/\\xa0소원\\xa0/\\xa0许愿  /  希望：为爱重生(台)', '十二怒汉\\xa0/\\xa012 Angry Men\\xa0/\\xa012怒汉  /  十二怒汉', '哈尔的移动城堡\\xa0/\\xa0ハウルの動く城\\xa0/\\xa0哈尔移动城堡(港)  /  霍尔的移动城堡(台)', '让子弹飞\\xa0/\\xa0让子弹飞一会儿  /  火烧云', '何以为家\\xa0/\\xa0كفرناحوم\\xa0/\\xa0迦百农  /  星仔打官司(港)', '摔跤吧！爸爸\\xa0/\\xa0Dangal\\xa0/\\xa0我和我的冠军女儿(台)  /  打死不离3父女(港)', '猫鼠游戏\\xa0/\\xa0Catch Me If You Can\\xa0/\\xa0逍遥法外  /  捉智双雄(港)', '天空之城\\xa0/\\xa0天空の城ラピュタ\\xa0/\\xa0天空之城拉普他  /  空中城堡拉普他', '鬼子来了\\xa0/\\xa0Devils on the Doorstep', '海蒂和爷爷\\xa0/\\xa0Heidi\\xa0/\\xa0飘零燕(港)  /  海蒂', '少年派的奇幻漂流\\xa0/\\xa0Life of Pi\\xa0/\\xa0少年Pi的奇幻漂流  /  漂流少年Pi', '钢琴家\\xa0/\\xa0The Pianist\\xa0/\\xa0钢琴战曲(港)  /  战地琴人(台)', '大话西游之月光宝盒\\xa0/\\xa0西遊記第壹佰零壹回之月光寶盒\\xa0/\\xa0西游记101回月光宝盒  /  齐天大圣东游记', '指环王2：双塔奇兵\\xa0/\\xa0The Lord of the Rings: The Two Towers\\xa0/\\xa0魔戒二部曲：双城奇谋  /  指环王II：双塔', '闻香识女人\\xa0/\\xa0Scent of a Woman\\xa0/\\xa0女人香  /  女人的芳香']\n",
      "['9.7', '9.6', '9.5', '9.5', '9.4', '9.4', '9.5', '9.4', '9.4', '9.5', '9.4', '9.4', '9.3', '9.2', '9.3', '9.3', '9.2', '9.3', '9.6', '9.2', '9.4', '9.3', '9.3', '9.2', '9.1', '9.3', '9.2', '9.1', '9.3', '9.2', '9.2', '9.3', '9.0', '9.3', '9.1', '9.3', '9.4', '9.1', '9.0', '9.1', '9.0', '9.1', '9.2', '9.3', '9.3', '9.1', '9.3', '9.0', '9.2', '9.1']\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "movie_name = []  # movie name\n",
    "movie_star = []  # movie star\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "# get movie name and star\n",
    "pages=2\n",
    "for i in range(pages):\n",
    "    names = soup.select('div.hd > a')\n",
    "    stars = soup.select('div.star > span.rating_num')\n",
    "    movie_names=[''.join([name.string for name in names[i].select('span')]) for i in range(len(names))]\n",
    "    movie_stars=[star.string for star in stars]\n",
    "    movie_name.extend(movie_names)\n",
    "    movie_star.extend(movie_stars)\n",
    "    next_page = soup.select('span.next')\n",
    "    url_next=next_page[0].a[\"href\"]\n",
    "    url_next=urllib.parse.urljoin(url,url_next)\n",
    "    page_next = requests.get(url_next, headers=headers)\n",
    "    soup = BeautifulSoup(page_next.content, 'html.parser')\n",
    "\n",
    "print(movie_name)\n",
    "print(movie_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) Collecting into a dataframe\n",
    "\n",
    "Create a dataframe `movie_df` and add the data from the lists above to it. \n",
    "- `movie_name` is the movie name. Set the column name as `movie name`\n",
    "- `movie_star` is the population estimate via star. Add it to the dataframe, and set the column name as `movie star`\n",
    "\n",
    "Make sure to check the head of your dataframe to see that everything looks right! ie: movie_df.head()\n",
    "\n",
    "Finally, you should save the DataFrame to a csv file under this folder`'./output'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name =  \"MovieDouban.csv\"\n",
    "csv_dir = \"./output\"\n",
    "\n",
    "# # YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "movie_df = pd.DataFrame({\"movie name\": movie_name, \"movie star\": movie_star})\n",
    "movie_df.head()\n",
    "movie_df.to_csv(csv_dir + '/' + csv_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: JSON File Analysis\n",
    "\n",
    "After the initial phase of web scraping, we transition to analyzing pre-collected data, which is often stored in accessible and structured formats like JSON and CSV. This approach allows us to bypass the time-consuming process of data collection through web scraping for certain datasets that are already available, enabling us to dive directly into data analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "In the section, you will first be working with a file called 'anon_user_dat.json'. You can find the given data under the folder `'./data/task2'`. This file contains information about some (fake) Tinder users. When creating an account, each Tinder user was asked to provide their first name, last name, work email (to verify the disclosed workplace), age, gender, phone # and zip code. Before releasing this data, a data scientist cleaned the data to protect the privacy of Tinder's users by removing the obvious personal identifiers: phone #, zip code, and IP address. However, the data scientist chose to keep each users' email addresses because when they visually skimmed a couple of the email addresses none of them seemed to have any of the users' actual names in them. This is where the data scientist made a huge mistake!\n",
    "\n",
    "Data Files:\n",
    "- anon_user_dat.json\n",
    "- employee_info.json\n",
    "\n",
    "\n",
    "We will take advantage of having the work email addresses by finding the employee information of different companies and matching that employee information with the information we have, in order to identify the names of the secret Tinder users!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) Load data from JSON file \n",
    "\n",
    "Load the `anon_user_dat.json` json file into a pandas dataframe. Call it `df_personal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>email</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>gshoreson0@seattletimes.com</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>eweaben1@salon.com</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>akillerby2@gravatar.com</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46</td>\n",
       "      <td>gsainz3@zdnet.com</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>bdanilewicz4@4shared.com</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age                        email  gender\n",
       "0   60  gshoreson0@seattletimes.com    Male\n",
       "1   47           eweaben1@salon.com  Female\n",
       "2   27      akillerby2@gravatar.com    Male\n",
       "3   46            gsainz3@zdnet.com    Male\n",
       "4   72     bdanilewicz4@4shared.com    Male"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df_personal = pd.read_json(\"data\\data_identifying\\\\anon_user_dat.json\")\n",
    "df_personal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df_personal, pd.DataFrame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Check the first 10 emails \n",
    "\n",
    "Save the first 10 emails to a Series, and call it `sample_emails`. \n",
    "You should then print out this Series. ( Use print() )\n",
    "\n",
    "The purpose of this is to get a sense of how these work emails are structured and how we could possibly extract where each anonymous user seems to work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    gshoreson0@seattletimes.com\n",
      "1             eweaben1@salon.com\n",
      "2        akillerby2@gravatar.com\n",
      "3              gsainz3@zdnet.com\n",
      "4       bdanilewicz4@4shared.com\n",
      "5      sdeerness5@wikispaces.com\n",
      "6         jstillwell6@ustream.tv\n",
      "7         mpriestland7@opera.com\n",
      "8       nerickssen8@hatena.ne.jp\n",
      "9             hparsell9@xing.com\n",
      "Name: email, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "sample_emails = df_personal['email'][:10]\n",
    "print(sample_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(sample_emails, pd.Series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) Extract the Company Name From the Email \n",
    "\n",
    "Create a function with the following specifications:\n",
    "- Function Name: extract_company\n",
    "- Purpose: to extract the company of the email (i.e., everything after the @ sign but before the first .)\n",
    "- Parameter(s): email (string)\n",
    "- Returns: The extracted part of the email (string)\n",
    "- Hint: This should take 1 line of code. Look into the find('') method. \n",
    "\n",
    "You can start with this outline:\n",
    "```python \n",
    "def extract_company(email):\n",
    "    return\n",
    "```\n",
    "\n",
    "Example Usage: \n",
    "- extract_company(\"larhe@uber.com\") should return \"uber\"\n",
    "- extract_company(“ds@cogs.edu”) should return “cogs”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "# extract company from email\n",
    "def extract_company(email):\n",
    "    return email[email.find(\"@\")+1:email.find(\".\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert extract_company(\"gshoreson0@seattletimes.com\") == \"seattletimes\"\n",
    "assert extract_company(\"amcgeffen1d@goo.ne.jp\") == 'goo'\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little bit of basic sleuthing (aka googling) and web-scraping (aka selectively reading in html code) it turns out that you've been able to collect information about all the present employees/interns of the companies you are interested in. Specifically, on each company website, you have found the name, gender, and age of its employees. You have saved that info in employee_info.json and plan to see if, using this new information, you can match the Tinder accounts to actual names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d) Load in employee data \n",
    "\n",
    "Load the json file into a pandas dataframe. Call it `df_employee`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df_employee=pd.read_json(\"data\\data_identifying\\employee_info.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df_employee, pd.DataFrame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e) Match the employee name with company, age, gender \n",
    "\n",
    "Create a function with the following specifications:\n",
    "- Function name: employee_matcher\n",
    "- Purpose: to match the employee name with the provided company, age, and gender\n",
    "- Parameter(s): company (string), age (int), gender (string)\n",
    "- Returns: The employee first_name and last_name like this: return first_name, last_name \n",
    "- Note: If there are multiple employees that fit the same description, first_name and last_name should return a list of all possible first names and last names i.e., ['Desmund', 'Kelby'], ['Shepley', 'Tichner']. Note that the names of the individuals that would produce this output are 'Desmund Shepley' and 'Kelby Tichner'.\n",
    "\n",
    "Hint:\n",
    "There are many different ways to code this. An inelegant solution is to loop through `df_employee` \n",
    "   and for each data item see if the company, age, and gender match\n",
    "   i.e., \n",
    "   ```python\n",
    "   for i in range(0, len(df_employee)):\n",
    "             if (company == df_employee.loc[i,'company']):\n",
    "   ```\n",
    "   \n",
    "However! The solution above is very inefficient and long, so you should try to look into this:\n",
    "Google the df.loc method: It extracts pieces of the dataframe\n",
    "   if it fulfills a certain condition.\n",
    "   i.e., \n",
    "   \n",
    "```python\n",
    "df_employee.loc[df_employee['company'] == company]\n",
    "```\n",
    "\n",
    "If you need to convert your pandas data series into a list, you can do ```list(result)``` where result is a pandas \"series\"\n",
    "\n",
    "You can start with this outline:\n",
    "```python\n",
    "def employee_matcher(company, age, gender):\n",
    "    return first_name, last_name\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "def employee_matcher(company, age, gender):\n",
    "        result=df_employee.loc[(df_employee['company'] == company)&(df_employee['age'] == age)&(df_employee['gender'] == gender)]\n",
    "        first_name=result.loc[:,'first_name'].to_list()\n",
    "        last_name=result.loc[:,'last_name'].to_list()\n",
    "        return first_name, last_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert employee_matcher(\"google\", 41, \"Male\") == (['Maxwell'], ['Jorio'])\n",
    "assert employee_matcher(\"salon\", 47, \"Female\") == (['Elenore'], ['Gravett'])\n",
    "assert employee_matcher(\"webmd\", 28, \"Nonbinary\") == (['Zaccaria'], ['Bartosiak'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2f) Extract all the private data \n",
    "\n",
    "- Create 2 empty lists called `first_names` and `last_names`\n",
    "- Loop through all the people we are trying to identify in df_personal\n",
    "- Call the `extract_company` function (i.e., `extract_company(df_personal.loc[i, 'email'])` )\n",
    "- Call the `employee_matcher` function \n",
    "- Append the results of `employee_matcher` to the appropriate lists (`first_names` and `last_names`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "first_names=[]\n",
    "last_names=[]\n",
    "for i in range(df_personal.shape[0]):\n",
    "    company=extract_company(df_personal.loc[i,'email'])\n",
    "    new_first_name,new_last_name=employee_matcher(company,df_personal.loc[i,'age'],df_personal.loc[i,'gender'])\n",
    "    first_names.append(new_first_name)\n",
    "    last_names.append(new_last_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert first_names[45:50]== [['Justino'], ['Tadio'], ['Kennith'], ['Cedric'], ['Amargo']]\n",
    "assert last_names[45:50] == [['Corro'], ['Blackford'], ['Milton'], ['Yggo'], ['Grigor']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2g) Add the names to the original 'secure' dataset! \n",
    "\n",
    "We have done this last step for you below, all you need to do is run this cell.\n",
    "\n",
    "For your own personal enjoyment, you should also print out the new `df_personal` with the identified people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personal['first_name'] = first_names\n",
    "df_personal['last_name'] = last_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Working with CSV Files\n",
    "Continuing with our exploration of pre-collected data formats, we delve into CSV files, which are renowned for their simplicity and widespread use in representing tabular data. This stage involves leveraging libraries like pandas in Python, which simplify the process of reading, manipulating, and analyzing CSV data. \n",
    "\n",
    "### Overview\n",
    "\n",
    "For this assignment, you are provided with two data files that contain information on a sample of people. The two files and their columns are:\n",
    "\n",
    "- `age_steps.csv`: Contains one row for each person.\n",
    "    - `id`: Unique identifier for the person.\n",
    "    - `age`: Age of the person.\n",
    "    - `steps`: Number of steps the person took on average in January 2018.\n",
    "    \n",
    "    \n",
    "- `incomes.json`: Contains one record for each person.\n",
    "    - `id`: Unique identifier for the person. Two records with the same ID between `age_steps.csv` and `incomes.json` correspond to the same person.\n",
    "    - `last_name`: Last name of the person.\n",
    "    - `first_name`: First name of the person.\n",
    "    - `income`: Income of the person in 2018.\n",
    "    \n",
    "You can find the given data under the folder `'./data/task3'`. To finish the assignment, we recommend looking at the official 10 minutes to pandas guide: http://pandas.pydata.org/pandas-docs/stable/10min.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3a:** Load the `age_steps.csv` file into a `pandas` DataFrame named `df_steps`. It should have 11257 rows and 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df_steps=pd.read_csv(\"data\\data_wrangling\\\\age_steps.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df_steps, pd.DataFrame)\n",
    "assert df_steps.shape == (11257, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3b:** Load the `incomes.json` file into a `pandas` DataFrame called `df_income`. The DataFrame should have 13332 rows and 4 columns. \n",
    "\n",
    "Hint: Find a pandas function similar to `read_csv` for JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df_income=pd.read_json(\"data\\data_wrangling\\incomes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df_income, pd.DataFrame)\n",
    "assert df_income.shape == (13332, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3c:** Drop the `first_name` and `last_name` columns from the `df_income` DataFrame. The resulting DataFrame should only have two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df_income.drop(columns=['first_name', 'last_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'first_name' not in df_income.columns\n",
    "assert 'last_name' not in df_income.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3d:** Merge the `df_steps` and `df_income` DataFrames into a single combined DataFrame called `df`. Use the `id` column to match rows together.\n",
    "\n",
    "The final DataFrame should have 10,135 rows and 4 columns: `id`, `income`, `age`, and `steps`.\n",
    "\n",
    "Call an appropriate `pandas` method to perform this operation; don't write a `for` loop. (In general, writing a `for` loop for a DataFrame will produce poor results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df=pd.merge(df_steps,df_income,on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df, pd.DataFrame)\n",
    "assert set(df.columns) == set(['id', 'income', 'age', 'steps'])\n",
    "assert df.shape == (10135, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3e:** Reorder the columns of `df` so that they appear in the order: `id`, `age`, `steps`, then `income`.\n",
    "\n",
    "(Note: If your DataFrame is already in this order, just put `df` in this cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>steps</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36859</td>\n",
       "      <td>48</td>\n",
       "      <td>6764</td>\n",
       "      <td>10056.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99794</td>\n",
       "      <td>39</td>\n",
       "      <td>4308</td>\n",
       "      <td>13869.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33364</td>\n",
       "      <td>36</td>\n",
       "      <td>6410</td>\n",
       "      <td>79634.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73874</td>\n",
       "      <td>35</td>\n",
       "      <td>7870</td>\n",
       "      <td>12369.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66956</td>\n",
       "      <td>56</td>\n",
       "      <td>7670</td>\n",
       "      <td>41150.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10130</th>\n",
       "      <td>42474</td>\n",
       "      <td>28</td>\n",
       "      <td>7307</td>\n",
       "      <td>49128.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10131</th>\n",
       "      <td>61626</td>\n",
       "      <td>44</td>\n",
       "      <td>7752</td>\n",
       "      <td>20096.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10132</th>\n",
       "      <td>52336</td>\n",
       "      <td>41</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10133</th>\n",
       "      <td>54972</td>\n",
       "      <td>44</td>\n",
       "      <td>7548</td>\n",
       "      <td>18350.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10134</th>\n",
       "      <td>17411</td>\n",
       "      <td>43</td>\n",
       "      <td>8765</td>\n",
       "      <td>88965.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10135 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  age  steps    income\n",
       "0      36859   48   6764  10056.43\n",
       "1      99794   39   4308  13869.47\n",
       "2      33364   36   6410  79634.92\n",
       "3      73874   35   7870  12369.03\n",
       "4      66956   56   7670  41150.18\n",
       "...      ...  ...    ...       ...\n",
       "10130  42474   28   7307  49128.60\n",
       "10131  61626   44   7752  20096.38\n",
       "10132  52336   41     -1      0.00\n",
       "10133  54972   44   7548  18350.20\n",
       "10134  17411   43   8765  88965.55\n",
       "\n",
       "[10135 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(df.columns) == ['id', 'age', 'steps', 'income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3f:** You may have noticed something strange: the merged `df` DataFrame has fewer rows than either of `df_steps` and `df_income`. Why did this happen? (If you're unsure, check out the documentation for the `pandas` method you used to merge these two datasets. Take note of the default values set for this method's parameters.)\n",
    "\n",
    "Please select the **one** correct explanation below and save your answer in the variable `q1f_answer`. For example, if you believe choice number 4 explains why `df` has fewer rows, set `q1f_answer = 4`.\n",
    "\n",
    "1. Some steps were recorded inaccurately in `df_steps`.\n",
    "2. Some incomes were recorded inaccurately in `df_income`.\n",
    "3. There are fewer rows in `df_steps` than in `df_income`.\n",
    "4. There are fewer columns in `df_steps` than in `df_income`.\n",
    "5. Some `id` values in either `df_steps` and `df_income` were missing in the other DataFrame.\n",
    "6. Some `id` values were repeated in `df_steps` and in `df_income`.\n",
    "\n",
    "You may use the cell below to run whatever code you want to check the statements above. Just make sure to set `q1f_answer` once you've selected a choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "q1f_answer=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(q1f_answer, int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Data Cleaning\n",
    "\n",
    "Post data collection, a pivotal step ensues—Data Cleaning. This phase is crucial for ensuring the reliability and accuracy of our analysis. It involves scrutinizing the data for inaccuracies, inconsistencies, and incompleteness. Techniques such as removing duplicates, handling missing values, and correcting errors are employed to refine the dataset. A common phenomenon is that the collected data may contain missing values. Here are two common ones:\n",
    "\n",
    "- **Nonresponse.** For example, people might have left a field blank when responding to a survey, or left the entire survey blank.\n",
    "- **Lost in entry.** Data might have been lost after initial recording. For example, a disk cleanup might accidentally wipe older entries of a database.\n",
    "\n",
    "In general, it is **not** appropriate to simply drop missing values from the dataset or pretend that if filled in they would not change your results. In 2016, many polls mistakenly predicted that Hillary Clinton would easily win the Presidential election by committing this error. In this particular dataset, however, the **missing values occur completely at random**. This criteria allows us to drop missing values without significantly affecting our conclusions.\n",
    "\n",
    "In this section, we continue use the data mentioned in Part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4a:** How many values are missing in the `income` column of `df`? Save this number into a variable called `n_nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "n_nan=df.loc[df[\"income\"].isnull()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(n_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4b:** Remove all rows from `df` that have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows from df that have missing data. In other words, remove all rows with NaN values.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(np.isnan(df['income'])) == 0\n",
    "assert df.shape == (9684, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4c:** Note that we can now compute the average income. If your `df` variable contains the right values, `df['income'].mean()` should produce the value `25508.84`.\n",
    "\n",
    "Suppose that we didn't drop the missing incomes. What will running `df['income'].mean()` output? Use the variable `q2c_answer` to record which of the below statements you think is true. As usual, you can use the cell below to run any code you'd like in order to help you answer this question as long as you set `q2c_answer` once you've finished.\n",
    "\n",
    "1. No change; `df['income'].mean()` will ignore the missing values and output `25508.84`.\n",
    "2. `df['income'].mean()` will produce an error.\n",
    "3. `df['income'].mean()` will output `0`.\n",
    "4. `df['income'].mean()` will output `nan` (not a number).\n",
    "5. `df['income'].mean()` will fill in the missing values with the average income, then compute the average.\n",
    "6. `df['income'].mean()` will fill in the missing values with `0`, then compute the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df['income'].mean()\n",
    "q2c_answer=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(q2c_answer, int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4d:** Suppose that missing incomes did not occur at random, and that individuals with incomes below \\$10000 a year are less likely to report their incomes. If so, which of the following statements below is true? Record your choice in the variable `q2d_answer`.\n",
    "\n",
    "1. `df['income'].mean()` will likely output a value that is the same as the population's average income\n",
    "2. `df['income'].mean()` will likely output a value that is smaller than the population's average income.\n",
    "3. `df['income'].mean()` will likely output a value that is larger than the population's average income.\n",
    "4. `df['income'].mean()` will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "q2d_answer=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(q2d_answer, int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete!\n",
    "\n",
    "Congrats, you're done!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
