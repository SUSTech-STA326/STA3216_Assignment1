{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA326 Assignment 1: Data Collection\n",
    "This is an assignment that is openly available for the Data Science Practice (STA326). \n",
    "\n",
    "The assignment encapsulates a holistic approach towards data collection and analysis, covering a spectrum of data formats and sources. Our objective is to amass, process, and scrutinize data to unearth significant insights. The methodology is sectioned into four pivotal tasks: \n",
    "- Web scraping \n",
    "- JSON file analysis \n",
    "- Working with CSV files\n",
    "- Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests  # send request\n",
    "from bs4 import BeautifulSoup  # parse web pages\n",
    "import pandas as pd  # csv\n",
    "from time import sleep  # wait\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web Scraping\n",
    "\n",
    "In this assignment, we will explore web scraping, which can often include diverse information from website, and also use the data for simple analysis. We take [douban](https://movie.douban.com/top250) as the target website in this assignment.\n",
    "\n",
    "### Scraping Rules\n",
    "\n",
    "1) If you are using another organization's website for scraping, make sure to check the website's terms & conditions. \n",
    "\n",
    "2) Do not request data from the website too aggressively (quickly) with your program (also known as spamming), as this may break the website. Make sure your program behaves in a reasonable manner (i.e. acts like a human). One request for one webpage per second is good practice.\n",
    "\n",
    "3) The layout of a website may change from time to time. Because of this, if you're scraping a website, make sure to revisit the site and rewrite your code as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Web Scrape\n",
    "\n",
    "In order to extract the data we want, we’ll start with extracting the whole web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a request header (to prevent anti-scraping)\n",
    "headers = {\n",
    "    'authority': 'curlconverter.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'if-modified-since': 'Fri, 15 Jul 2022 21:44:42 GMT',\n",
    "    'if-none-match': 'W/\"62d1dfca-3a58\"',\n",
    "    'referer': 'https://curlconverter.com/',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Microsoft Edge\";v=\"102\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Linux\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'cross-site',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process can be split into three steps:\n",
    "\n",
    "1. Make a variable called `url`, that stores the following URL (as a string):\n",
    "https://movie.douban.com/top250?start=0\n",
    "\n",
    "2. Now, to open the URL, use `requests.get()` and provide `url` and `headers` as its input. Store this in a variable called `page`.\n",
    "\n",
    "3. After that, make a variable called `soup` to parse the HTML using `BeautifulSoup`. Consider that there will be a method from `BeautifulSoup` that you'll need to call on to get the content from the page. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "url = \"https://movie.douban.com/top250?start=0\"\n",
    "page = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert url\n",
    "assert page\n",
    "assert soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Data Extraction\n",
    "\n",
    "Extract the data (`name` and `star`) from the page and save it in the corresponding list like `movie_name` and `movie_star`. \n",
    "\n",
    "Make sure you extract it as a string.\n",
    "\n",
    "To do so, you have to use the soup object created in the above cell.\n",
    "\n",
    "**Hint**: from your soup variable, you can access this with `soup.select()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['肖申克的救赎 / The Shawshank Redemption / 月黑高飞(港)  /  刺激1995(台)', '霸王别姬 / 再见，我的妾  /  Farewell My Concubine', '阿甘正传 / Forrest Gump / 福雷斯特·冈普', '泰坦尼克号 / Titanic / 铁达尼号(港 / 台)', '千与千寻 / 千と千尋の神隠し / 神隐少女(台)  /  千与千寻的神隐', '这个杀手不太冷 / Léon / 终极追杀令(台)  /  杀手莱昂', '美丽人生 / La vita è bella / 一个快乐的传说(港)  /  Life Is Beautiful', '星际穿越 / Interstellar / 星际启示录(港)  /  星际效应(台)', '盗梦空间 / Inception / 潜行凶间(港)  /  全面启动(台)', \"辛德勒的名单 / Schindler's List / 舒特拉的名单(港)  /  辛德勒名单\", '楚门的世界 / The Truman Show / 真人Show(港)  /  真人戏', \"忠犬八公的故事 / Hachi: A Dog's Tale / 秋田犬八千(港)  /  忠犬小八(台)\", \"海上钢琴师 / La leggenda del pianista sull'oceano / 声光伴我飞(港)  /  一九零零的传奇\", '三傻大闹宝莱坞 / 3 Idiots / 三个傻瓜(台)  /  作死不离3兄弟(港)', '放牛班的春天 / Les choristes / 歌声伴我心(港)  /  唱诗班男孩', '机器人总动员 / WALL·E / 太空奇兵·威E(港)  /  瓦力(台)', '疯狂动物城 / Zootopia / 优兽大都会(港)  /  动物方城市(台)', '无间道 / 無間道 / Infernal Affairs  /  Mou gaan dou', '控方证人 / Witness for the Prosecution / 雄才伟略  /  情妇', '大话西游之大圣娶亲 / 西遊記大結局之仙履奇緣 / 西游记完结篇仙履奇缘  /  齐天大圣西游记', '熔炉 / 도가니 / 无声呐喊(港)  /  漩涡', \"教父 / The Godfather / Mario Puzo's The Godfather\", '触不可及 / Intouchables / 闪亮人生(港)  /  逆转人生(台)', '当幸福来敲门 / The Pursuit of Happyness / 寻找快乐的故事(港)  /  追求快乐', '寻梦环游记 / Coco / 玩转极乐园(港)  /  可可夜总会(台)']\n",
      "['9.7', '9.6', '9.5', '9.5', '9.4', '9.4', '9.5', '9.4', '9.4', '9.5', '9.4', '9.4', '9.3', '9.2', '9.3', '9.3', '9.2', '9.3', '9.6', '9.2', '9.4', '9.3', '9.3', '9.2', '9.1']\n"
     ]
    }
   ],
   "source": [
    "movie_name = []  # movie name\n",
    "movie_star = []  # movie star\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "for li in soup.select('ol.grid_view li'):   \n",
    "    title_spans = li.select('.info .hd a .title') + li.select('.info .hd a .other')\n",
    "    title = ' '.join(span.text.strip().replace('\\xa0', ' ') for span in title_spans)\n",
    "    rating = li.select_one('.info .bd .star .rating_num').text.strip()\n",
    "    movie_name.append(title)\n",
    "    movie_star.append(rating)\n",
    "print(movie_name)\n",
    "print(movie_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) Collecting into a dataframe\n",
    "\n",
    "Create a dataframe `movie_df` and add the data from the lists above to it. \n",
    "- `movie_name` is the movie name. Set the column name as `movie name`\n",
    "- `movie_star` is the population estimate via star. Add it to the dataframe, and set the column name as `movie star`\n",
    "\n",
    "Make sure to check the head of your dataframe to see that everything looks right! ie: movie_df.head()\n",
    "\n",
    "Finally, you should save the DataFrame to a csv file under this folder`'./output'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          movie name movie star\n",
      "0  肖申克的救赎 / The Shawshank Redemption / 月黑高飞(港)  /...        9.7\n",
      "1            霸王别姬 / 再见，我的妾  /  Farewell My Concubine        9.6\n",
      "2                      阿甘正传 / Forrest Gump / 福雷斯特·冈普        9.5\n",
      "3                      泰坦尼克号 / Titanic / 铁达尼号(港 / 台)        9.5\n",
      "4              千与千寻 / 千と千尋の神隠し / 神隐少女(台)  /  千与千寻的神隐        9.4\n"
     ]
    }
   ],
   "source": [
    "csv_name =  \"MovieDouban.csv\"\n",
    "csv_dir = \"./output\"\n",
    "\n",
    "# # YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "movie = pd.DataFrame({\"movie name\": movie_name, \"movie star\": movie_star})\n",
    "print(movie.head())\n",
    "movie.to_csv(f\"{csv_dir}/{csv_name}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          movie name movie star\n",
      "0  肖申克的救赎 / The Shawshank Redemption / 月黑高飞(港)  /...        9.7\n",
      "1            霸王别姬 / 再见，我的妾  /  Farewell My Concubine        9.6\n",
      "2                      阿甘正传 / Forrest Gump / 福雷斯特·冈普        9.5\n",
      "3                      泰坦尼克号 / Titanic / 铁达尼号(港 / 台)        9.5\n",
      "4              千与千寻 / 千と千尋の神隠し / 神隐少女(台)  /  千与千寻的神隐        9.4\n"
     ]
    }
   ],
   "source": [
    "# Find the Top 250 Movies on Douban\n",
    "movie_name = []  \n",
    "movie_star = [] \n",
    "base_url = \"https://movie.douban.com/top250?start={}\"\n",
    "for i in range(0, 226, 25):\n",
    "    url = base_url.format(i)\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser') \n",
    "    for li in soup.select('ol.grid_view li'):\n",
    "        title_spans = li.select('.info .hd a .title') + li.select('.info .hd a .other')\n",
    "        title = ' '.join(span.text.strip().replace('\\xa0', ' ') for span in title_spans)\n",
    "        rating = li.select_one('.info .bd .star .rating_num').text.strip()\n",
    "        movie_name.append(title)\n",
    "        movie_star.append(rating)\n",
    "    sleep(5)\n",
    "movie = pd.DataFrame({\"movie name\": movie_name, \"movie star\": movie_star})\n",
    "print(movie.head())\n",
    "movie.to_csv(f\"{csv_dir}/MovieDouban250.csv\", index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: JSON File Analysis\n",
    "\n",
    "After the initial phase of web scraping, we transition to analyzing pre-collected data, which is often stored in accessible and structured formats like JSON and CSV. This approach allows us to bypass the time-consuming process of data collection through web scraping for certain datasets that are already available, enabling us to dive directly into data analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "In the section, you will first be working with a file called 'anon_user_dat.json'. You can find the given data under the folder `'./data/task2'`. This file contains information about some (fake) Tinder users. When creating an account, each Tinder user was asked to provide their first name, last name, work email (to verify the disclosed workplace), age, gender, phone # and zip code. Before releasing this data, a data scientist cleaned the data to protect the privacy of Tinder's users by removing the obvious personal identifiers: phone #, zip code, and IP address. However, the data scientist chose to keep each users' email addresses because when they visually skimmed a couple of the email addresses none of them seemed to have any of the users' actual names in them. This is where the data scientist made a huge mistake!\n",
    "\n",
    "Data Files:\n",
    "- anon_user_dat.json\n",
    "- employee_info.json\n",
    "\n",
    "\n",
    "We will take advantage of having the work email addresses by finding the employee information of different companies and matching that employee information with the information we have, in order to identify the names of the secret Tinder users!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) Load data from JSON file \n",
    "\n",
    "Load the `anon_user_dat.json` json file into a pandas dataframe. Call it `df_personal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age                        email  gender\n",
      "0   60  gshoreson0@seattletimes.com    Male\n",
      "1   47           eweaben1@salon.com  Female\n",
      "2   27      akillerby2@gravatar.com    Male\n",
      "3   46            gsainz3@zdnet.com    Male\n",
      "4   72     bdanilewicz4@4shared.com    Male\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "df_personal = pd.read_json('./data/task2/anon_user_dat.json')\n",
    "print(df_personal.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Check the first 10 emails \n",
    "\n",
    "Save the first 10 emails to a Series, and call it `sample_emails`. \n",
    "You should then print out this Series. ( Use print() )\n",
    "\n",
    "The purpose of this is to get a sense of how these work emails are structured and how we could possibly extract where each anonymous user seems to work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df_personal, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    gshoreson0@seattletimes.com\n",
      "1             eweaben1@salon.com\n",
      "2        akillerby2@gravatar.com\n",
      "3              gsainz3@zdnet.com\n",
      "4       bdanilewicz4@4shared.com\n",
      "5      sdeerness5@wikispaces.com\n",
      "6         jstillwell6@ustream.tv\n",
      "7         mpriestland7@opera.com\n",
      "8       nerickssen8@hatena.ne.jp\n",
      "9             hparsell9@xing.com\n",
      "Name: email, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "sample_emails = df_personal['email'].head(10)\n",
    "print(sample_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(sample_emails, pd.Series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) Extract the Company Name From the Email \n",
    "\n",
    "Create a function with the following specifications:\n",
    "- Function Name: extract_company\n",
    "- Purpose: to extract the company of the email (i.e., everything after the @ sign but before the first .)\n",
    "- Parameter(s): email (string)\n",
    "- Returns: The extracted part of the email (string)\n",
    "- Hint: This should take 1 line of code. Look into the find('') method. \n",
    "\n",
    "You can start with this outline:\n",
    "```python \n",
    "def extract_company(email):\n",
    "    return\n",
    "```\n",
    "\n",
    "Example Usage: \n",
    "- extract_company(\"larhe@uber.com\") should return \"uber\"\n",
    "- extract_company(“ds@cogs.edu”) should return “cogs”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "def extract_company(email):\n",
    "    return email.split(\"@\")[1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert extract_company(\"gshoreson0@seattletimes.com\") == \"seattletimes\"\n",
    "assert extract_company(\"amcgeffen1d@goo.ne.jp\") == 'goo'\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little bit of basic sleuthing (aka googling) and web-scraping (aka selectively reading in html code) it turns out that you've been able to collect information about all the present employees/interns of the companies you are interested in. Specifically, on each company website, you have found the name, gender, and age of its employees. You have saved that info in employee_info.json and plan to see if, using this new information, you can match the Tinder accounts to actual names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d) Load in employee data \n",
    "\n",
    "Load the json file into a pandas dataframe. Call it `df_employee`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   company first_name last_name  gender  age\n",
      "0  123-reg  Inglebert  Falconer    Male   42\n",
      "1      163     Rafael  Bedenham    Male   14\n",
      "2      163     Lemuel      Lind    Male   31\n",
      "3      163      Penny   Pennone  Female   45\n",
      "4      163       Elva  Crighton  Female   52\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df_employee = pd.read_json('./data/task2/employee_info.json')\n",
    "print(df_employee.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df_employee, pd.DataFrame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e) Match the employee name with company, age, gender \n",
    "\n",
    "Create a function with the following specifications:\n",
    "- Function name: employee_matcher\n",
    "- Purpose: to match the employee name with the provided company, age, and gender\n",
    "- Parameter(s): company (string), age (int), gender (string)\n",
    "- Returns: The employee first_name and last_name like this: return first_name, last_name \n",
    "- Note: If there are multiple employees that fit the same description, first_name and last_name should return a list of all possible first names and last names i.e., ['Desmund', 'Kelby'], ['Shepley', 'Tichner']. Note that the names of the individuals that would produce this output are 'Desmund Shepley' and 'Kelby Tichner'.\n",
    "\n",
    "Hint:\n",
    "There are many different ways to code this. An inelegant solution is to loop through `df_employee` \n",
    "   and for each data item see if the company, age, and gender match\n",
    "   i.e., \n",
    "   ```python\n",
    "   for i in range(0, len(df_employee)):\n",
    "             if (company == df_employee.loc[i,'company']):\n",
    "   ```\n",
    "   \n",
    "However! The solution above is very inefficient and long, so you should try to look into this:\n",
    "Google the df.loc method: It extracts pieces of the dataframe\n",
    "   if it fulfills a certain condition.\n",
    "   i.e., \n",
    "   \n",
    "```python\n",
    "df_employee.loc[df_employee['company'] == company]\n",
    "```\n",
    "\n",
    "If you need to convert your pandas data series into a list, you can do ```list(result)``` where result is a pandas \"series\"\n",
    "\n",
    "You can start with this outline:\n",
    "```python\n",
    "def employee_matcher(company, age, gender):\n",
    "    return first_name, last_name\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "def employee_matcher(company, age, gender):\n",
    "    matched = df_employee.loc[(df_employee['company'] == company) & (df_employee['age'] == age) & (df_employee['gender'] == gender)]\n",
    "    first_name = list(matched['first_name'])\n",
    "    last_name = list(matched['last_name'])\n",
    "    return first_name, last_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert employee_matcher(\"google\", 41, \"Male\") == (['Maxwell'], ['Jorio'])\n",
    "assert employee_matcher(\"salon\", 47, \"Female\") == (['Elenore'], ['Gravett'])\n",
    "assert employee_matcher(\"webmd\", 28, \"Nonbinary\") == (['Zaccaria'], ['Bartosiak'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2f) Extract all the private data \n",
    "\n",
    "- Create 2 empty lists called `first_names` and `last_names`\n",
    "- Loop through all the people we are trying to identify in df_personal\n",
    "- Call the `extract_company` function (i.e., `extract_company(df_personal.loc[i, 'email'])` )\n",
    "- Call the `employee_matcher` function \n",
    "- Append the results of `employee_matcher` to the appropriate lists (`first_names` and `last_names`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "first_names = []\n",
    "last_names = []\n",
    "for i in range(len(df_personal)):\n",
    "    company = extract_company(df_personal.loc[i, 'email'])\n",
    "    matched_first_names, matched_last_names = employee_matcher(company, df_personal.loc[i, 'age'], df_personal.loc[i, 'gender'])\n",
    "    first_names.append(matched_first_names)\n",
    "    last_names.append(matched_last_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert first_names[45:50]== [['Justino'], ['Tadio'], ['Kennith'], ['Cedric'], ['Amargo']]\n",
    "assert last_names[45:50] == [['Corro'], ['Blackford'], ['Milton'], ['Yggo'], ['Grigor']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2g) Add the names to the original 'secure' dataset! \n",
    "\n",
    "We have done this last step for you below, all you need to do is run this cell.\n",
    "\n",
    "For your own personal enjoyment, you should also print out the new `df_personal` with the identified people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personal['first_name'] = first_names\n",
    "df_personal['last_name'] = last_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age                        email  gender           first_name  \\\n",
      "0     60  gshoreson0@seattletimes.com    Male             [Gordon]   \n",
      "1     47           eweaben1@salon.com  Female            [Elenore]   \n",
      "2     27      akillerby2@gravatar.com    Male               [Abbe]   \n",
      "3     46            gsainz3@zdnet.com    Male              [Guido]   \n",
      "4     72     bdanilewicz4@4shared.com    Male              [Brody]   \n",
      "..   ...                          ...     ...                  ...   \n",
      "995    3        pstroulgerrn@time.com  Female           [Penelopa]   \n",
      "996   49  kbasnettro@seattletimes.com  Female  [Anthiathia, Kandy]   \n",
      "997   75  pmortlockrp@liveinternet.ru    Male               [Paco]   \n",
      "998   81         sphetterq@toplist.cz    Male              [Sammy]   \n",
      "999   70        jtyresrr@slashdot.org    Male             [Josiah]   \n",
      "\n",
      "             last_name  \n",
      "0          [DelaField]  \n",
      "1            [Gravett]  \n",
      "2          [Stockdale]  \n",
      "3            [Comfort]  \n",
      "4           [Pinckard]  \n",
      "..                 ...  \n",
      "995            [Roman]  \n",
      "996  [Baldwin, Cossam]  \n",
      "997      [Weatherburn]  \n",
      "998           [Dymick]  \n",
      "999         [Ayshford]  \n",
      "\n",
      "[1000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_personal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Working with CSV Files\n",
    "Continuing with our exploration of pre-collected data formats, we delve into CSV files, which are renowned for their simplicity and widespread use in representing tabular data. This stage involves leveraging libraries like pandas in Python, which simplify the process of reading, manipulating, and analyzing CSV data. \n",
    "\n",
    "### Overview\n",
    "\n",
    "For this assignment, you are provided with two data files that contain information on a sample of people. The two files and their columns are:\n",
    "\n",
    "- `age_steps.csv`: Contains one row for each person.\n",
    "    - `id`: Unique identifier for the person.\n",
    "    - `age`: Age of the person.\n",
    "    - `steps`: Number of steps the person took on average in January 2018.\n",
    "    \n",
    "    \n",
    "- `incomes.json`: Contains one record for each person.\n",
    "    - `id`: Unique identifier for the person. Two records with the same ID between `age_steps.csv` and `incomes.json` correspond to the same person.\n",
    "    - `last_name`: Last name of the person.\n",
    "    - `first_name`: First name of the person.\n",
    "    - `income`: Income of the person in 2018.\n",
    "    \n",
    "You can find the given data under the folder `'./data/task3'`. To finish the assignment, we recommend looking at the official 10 minutes to pandas guide: http://pandas.pydata.org/pandas-docs/stable/10min.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3a:** Load the `age_steps.csv` file into a `pandas` DataFrame named `df_steps`. It should have 11257 rows and 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df_steps = pd.read_csv('./data/task3/age_steps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df_steps, pd.DataFrame)\n",
    "assert df_steps.shape == (11257, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3b:** Load the `incomes.json` file into a `pandas` DataFrame called `df_income`. The DataFrame should have 13332 rows and 4 columns. \n",
    "\n",
    "Hint: Find a pandas function similar to `read_csv` for JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df_income = pd.read_json('./data/task3/incomes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df_income, pd.DataFrame)\n",
    "assert df_income.shape == (13332, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3c:** Drop the `first_name` and `last_name` columns from the `df_income` DataFrame. The resulting DataFrame should only have two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df_income.drop(columns=['first_name', 'last_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'first_name' not in df_income.columns\n",
    "assert 'last_name' not in df_income.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3d:** Merge the `df_steps` and `df_income` DataFrames into a single combined DataFrame called `df`. Use the `id` column to match rows together.\n",
    "\n",
    "The final DataFrame should have 10,135 rows and 4 columns: `id`, `income`, `age`, and `steps`.\n",
    "\n",
    "Call an appropriate `pandas` method to perform this operation; don't write a `for` loop. (In general, writing a `for` loop for a DataFrame will produce poor results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df = pd.merge(df_income, df_steps, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df, pd.DataFrame)\n",
    "assert set(df.columns) == set(['id', 'income', 'age', 'steps'])\n",
    "assert df.shape == (10135, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3e:** Reorder the columns of `df` so that they appear in the order: `id`, `age`, `steps`, then `income`.\n",
    "\n",
    "(Note: If your DataFrame is already in this order, just put `df` in this cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>steps</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84764</td>\n",
       "      <td>41</td>\n",
       "      <td>8622</td>\n",
       "      <td>99807.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49337</td>\n",
       "      <td>31</td>\n",
       "      <td>9870</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41693</td>\n",
       "      <td>37</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98170</td>\n",
       "      <td>34</td>\n",
       "      <td>6987</td>\n",
       "      <td>18077.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3319</td>\n",
       "      <td>28</td>\n",
       "      <td>6976</td>\n",
       "      <td>18090.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10130</th>\n",
       "      <td>56013</td>\n",
       "      <td>23</td>\n",
       "      <td>7227</td>\n",
       "      <td>3715.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10131</th>\n",
       "      <td>21142</td>\n",
       "      <td>28</td>\n",
       "      <td>12159</td>\n",
       "      <td>3826.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10132</th>\n",
       "      <td>68473</td>\n",
       "      <td>40</td>\n",
       "      <td>7895</td>\n",
       "      <td>7617.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10133</th>\n",
       "      <td>60486</td>\n",
       "      <td>49</td>\n",
       "      <td>6004</td>\n",
       "      <td>34479.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10134</th>\n",
       "      <td>13915</td>\n",
       "      <td>39</td>\n",
       "      <td>7180</td>\n",
       "      <td>12133.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10135 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  age  steps    income\n",
       "0      84764   41   8622  99807.16\n",
       "1      49337   31   9870      0.00\n",
       "2      41693   37     -1      0.00\n",
       "3      98170   34   6987  18077.78\n",
       "4       3319   28   6976  18090.91\n",
       "...      ...  ...    ...       ...\n",
       "10130  56013   23   7227   3715.70\n",
       "10131  21142   28  12159   3826.20\n",
       "10132  68473   40   7895   7617.27\n",
       "10133  60486   49   6004  34479.99\n",
       "10134  13915   39   7180  12133.79\n",
       "\n",
       "[10135 rows x 4 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df = df[['id', 'age', 'steps', 'income']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(df.columns) == ['id', 'age', 'steps', 'income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3f:** You may have noticed something strange: the merged `df` DataFrame has fewer rows than either of `df_steps` and `df_income`. Why did this happen? (If you're unsure, check out the documentation for the `pandas` method you used to merge these two datasets. Take note of the default values set for this method's parameters.)\n",
    "\n",
    "Please select the **one** correct explanation below and save your answer in the variable `q1f_answer`. For example, if you believe choice number 4 explains why `df` has fewer rows, set `q1f_answer = 4`.\n",
    "\n",
    "1. Some steps were recorded inaccurately in `df_steps`.\n",
    "2. Some incomes were recorded inaccurately in `df_income`.\n",
    "3. There are fewer rows in `df_steps` than in `df_income`.\n",
    "4. There are fewer columns in `df_steps` than in `df_income`.\n",
    "5. Some `id` values in either `df_steps` and `df_income` were missing in the other DataFrame.\n",
    "6. Some `id` values were repeated in `df_steps` and in `df_income`.\n",
    "\n",
    "You may use the cell below to run whatever code you want to check the statements above. Just make sure to set `q1f_answer` once you've selected a choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "q1f_answer = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(q1f_answer, int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Data Cleaning\n",
    "\n",
    "Post data collection, a pivotal step ensues—Data Cleaning. This phase is crucial for ensuring the reliability and accuracy of our analysis. It involves scrutinizing the data for inaccuracies, inconsistencies, and incompleteness. Techniques such as removing duplicates, handling missing values, and correcting errors are employed to refine the dataset. A common phenomenon is that the collected data may contain missing values. Here are two common ones:\n",
    "\n",
    "- **Nonresponse.** For example, people might have left a field blank when responding to a survey, or left the entire survey blank.\n",
    "- **Lost in entry.** Data might have been lost after initial recording. For example, a disk cleanup might accidentally wipe older entries of a database.\n",
    "\n",
    "In general, it is **not** appropriate to simply drop missing values from the dataset or pretend that if filled in they would not change your results. In 2016, many polls mistakenly predicted that Hillary Clinton would easily win the Presidential election by committing this error. In this particular dataset, however, the **missing values occur completely at random**. This criteria allows us to drop missing values without significantly affecting our conclusions.\n",
    "\n",
    "In this section, we continue use the data mentioned in Part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4a:** How many values are missing in the `income` column of `df`? Save this number into a variable called `n_nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "n_nan = df['income'].isna().sum()\n",
    "n_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(n_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4b:** Remove all rows from `df` that have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows from df that have missing data. In other words, remove all rows with NaN values.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(np.isnan(df['income'])) == 0\n",
    "assert df.shape == (9684, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4c:** Note that we can now compute the average income. If your `df` variable contains the right values, `df['income'].mean()` should produce the value `25508.84`.\n",
    "\n",
    "Suppose that we didn't drop the missing incomes. What will running `df['income'].mean()` output? Use the variable `q2c_answer` to record which of the below statements you think is true. As usual, you can use the cell below to run any code you'd like in order to help you answer this question as long as you set `q2c_answer` once you've finished.\n",
    "\n",
    "1. No change; `df['income'].mean()` will ignore the missing values and output `25508.84`.\n",
    "2. `df['income'].mean()` will produce an error.\n",
    "3. `df['income'].mean()` will output `0`.\n",
    "4. `df['income'].mean()` will output `nan` (not a number).\n",
    "5. `df['income'].mean()` will fill in the missing values with the average income, then compute the average.\n",
    "6. `df['income'].mean()` will fill in the missing values with `0`, then compute the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  25508.838711276334\n",
      "before:  25508.83871127633\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "print(\"mean: \", df['income'].mean())\n",
    "df_test = pd.merge(df_income, df_steps, on='id')\n",
    "print(\"before: \", df_test['income'].mean())\n",
    "q2c_answer = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(q2c_answer, int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4d:** Suppose that missing incomes did not occur at random, and that individuals with incomes below \\$10000 a year are less likely to report their incomes. If so, which of the following statements below is true? Record your choice in the variable `q2d_answer`.\n",
    "\n",
    "1. `df['income'].mean()` will likely output a value that is the same as the population's average income\n",
    "2. `df['income'].mean()` will likely output a value that is smaller than the population's average income.\n",
    "3. `df['income'].mean()` will likely output a value that is larger than the population's average income.\n",
    "4. `df['income'].mean()` will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "q2d_answer = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(q2d_answer, int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete!\n",
    "\n",
    "Congrats, you're done!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
