{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA326 Assignment 1: Data Collection\n",
    "This is an assignment that is openly available for the Data Science Practice (STA326). \n",
    "\n",
    "The assignment encapsulates a holistic approach towards data collection and analysis, covering a spectrum of data formats and sources. Our objective is to amass, process, and scrutinize data to unearth significant insights. The methodology is sectioned into four pivotal tasks: \n",
    "- Web scraping \n",
    "- JSON file analysis \n",
    "- Working with CSV files\n",
    "- Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests  # send request\n",
    "from bs4 import BeautifulSoup  # parse web pages\n",
    "import pandas as pd  # csv\n",
    "from time import sleep  # wait\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web Scraping\n",
    "\n",
    "In this assignment, we will explore web scraping, which can often include diverse information from website, and also use the data for simple analysis. We take [douban](https://movie.douban.com/top250) as the target website in this assignment.\n",
    "\n",
    "### Scraping Rules\n",
    "\n",
    "1) If you are using another organization's website for scraping, make sure to check the website's terms & conditions. \n",
    "\n",
    "2) Do not request data from the website too aggressively (quickly) with your program (also known as spamming), as this may break the website. Make sure your program behaves in a reasonable manner (i.e. acts like a human). One request for one webpage per second is good practice.\n",
    "\n",
    "3) The layout of a website may change from time to time. Because of this, if you're scraping a website, make sure to revisit the site and rewrite your code as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Web Scrape\n",
    "\n",
    "In order to extract the data we want, we’ll start with extracting the whole web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a request header (to prevent anti-scraping)\n",
    "headers = {\n",
    "    'authority': 'curlconverter.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'if-modified-since': 'Fri, 15 Jul 2022 21:44:42 GMT',\n",
    "    'if-none-match': 'W/\"62d1dfca-3a58\"',\n",
    "    'referer': 'https://curlconverter.com/',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Microsoft Edge\";v=\"102\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Linux\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'cross-site',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process can be split into three steps:\n",
    "\n",
    "1. Make a variable called `url`, that stores the following URL (as a string):\n",
    "https://movie.douban.com/top250?start=0\n",
    "\n",
    "2. Now, to open the URL, use `requests.get()` and provide `url` and `headers` as its input. Store this in a variable called `page`.\n",
    "\n",
    "3. After that, make a variable called `soup` to parse the HTML using `BeautifulSoup`. Consider that there will be a method from `BeautifulSoup` that you'll need to call on to get the content from the page. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://movie.douban.com/top250\"\n",
    "page=url\n",
    "response = requests.get(page, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# Now you can work with the soup object to extract information from the HTML page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert url\n",
    "assert page\n",
    "assert soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Data Extraction\n",
    "\n",
    "Extract the data (`name` and `star`) from the page and save it in the corresponding list like `movie_name` and `movie_star`. \n",
    "\n",
    "Make sure you extract it as a string.\n",
    "\n",
    "To do so, you have to use the soup object created in the above cell.\n",
    "\n",
    "**Hint**: from your soup variable, you can access this with `soup.select()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['肖申克的救赎', '霸王别姬', '阿甘正传', '泰坦尼克号', '千与千寻', '这个杀手不太冷', '美丽人生', '星际穿越', '盗梦空间', '辛德勒的名单', '楚门的世界', '忠犬八公的故事', '海上钢琴师', '三傻大闹宝莱坞', '放牛班的春天', '机器人总动员', '疯狂动物城', '无间道', '控方证人', '大话西游之大圣娶亲', '熔炉', '教父', '触不可及', '当幸福来敲门', '寻梦环游记', '末代皇帝', '龙猫', '怦然心动', '活着', '哈利·波特与魔法石', '蝙蝠侠：黑暗骑士', '指环王3：王者无敌', '我不是药神', '乱世佳人', '飞屋环游记', '素媛', '十二怒汉', '哈尔的移动城堡', '让子弹飞', '何以为家', '摔跤吧！爸爸', '猫鼠游戏', '天空之城', '鬼子来了', '海蒂和爷爷', '少年派的奇幻漂流', '钢琴家', '大话西游之月光宝盒', '指环王2：双塔奇兵', '闻香识女人', '死亡诗社', '绿皮书', '罗马假日', '大闹天宫', '天堂电影院', '黑客帝国', '指环王1：护戒使者', '教父2', '狮子王', '辩护人', '饮食男女', '搏击俱乐部', '本杰明·巴顿奇事', '美丽心灵', '穿条纹睡衣的男孩', '窃听风暴', '情书', '两杆大烟枪', '西西里的美丽传说', '音乐之声', '看不见的客人', '哈利·波特与死亡圣器(下)', '阿凡达', '拯救大兵瑞恩', '飞越疯人院', '小鞋子', '沉默的羔羊', '功夫', '布达佩斯大饭店', '禁闭岛', '蝴蝶效应', '哈利·波特与阿兹卡班的囚徒', '致命魔术', '心灵捕手', '超脱', '低俗小说', '摩登时代', '海豚湾', '春光乍泄', '喜剧之王', '美国往事', '致命ID', '杀人回忆', '哈利·波特与密室', '红辣椒', '加勒比海盗', '七宗罪', '一一', '狩猎', '唐伯虎点秋香', '7号房的礼物', '被嫌弃的松子的一生', '蝙蝠侠：黑暗骑士崛起', '爱在黎明破晓前', '请以你的名字呼唤我', '甜蜜蜜', '断背山', '剪刀手爱德华', '入殓师', '第六感', '重庆森林', '超能陆战队', '勇敢的心', '爱在日落黄昏时', '幽灵公主', '菊次郎的夏天', '寄生虫', '借东西的小人阿莉埃蒂', '消失的爱人', '无人知晓', '阳光灿烂的日子', '完美的世界', '倩女幽魂', '天使爱美丽', '时空恋旅人', '小森林 夏秋篇', '未麻的部屋', '哈利·波特与火焰杯', '侧耳倾听', '驯龙高手', '幸福终点站', '一个叫欧维的男人决定去死', '怪兽电力公司', '教父3', '小森林 冬春篇', '玩具总动员3', '新世界', '傲慢与偏见', '茶馆', '萤火之森', '釜山行', '被解救的姜戈', '神偷奶爸', '色，戒', '告白', '哪吒闹海', '九品芝麻官', '玛丽和马克思', '喜宴', '大鱼', '模仿游戏', '头号玩家', '花样年华', '头脑特工队', '射雕英雄传之东成西就', '我是山姆', '七武士', '阳光姐妹淘', '血战钢锯岭', '惊魂记', '恐怖直播', '黑客帝国3：矩阵革命', '你的名字。', '三块广告牌', '电锯惊魂', '心迷宫', '达拉斯买家俱乐部', '疯狂原始人', '谍影重重3', '英雄本色', '上帝之城', '小丑', '绿里奇迹', '风之谷', '爱在午夜降临前', '海街日记', '纵横四海', '卢旺达饭店', '心灵奇旅', '背靠背，脸对脸', '疯狂的石头', '记忆碎片', '雨中曲', '2001太空漫游', '无间道2', '岁月神偷', '小偷家族', '忠犬八公物语', '无敌破坏王', '荒蛮故事', '冰川时代', '爆裂鼓手', '恐怖游轮', '牯岭街少年杀人事件', '东京教父', '贫民窟的百万富翁', '大佛普拉斯', '魔女宅急便', '东邪西毒', '遗愿清单', '你看起来好像很好吃', '可可西里', '黑天鹅', '城市之光', '源代码', '真爱至上', '海边的曼彻斯特', '末路狂花', '波西米亚狂想曲', '雨人', '青蛇', '初恋这件小事', '人工智能', '爱乐之城', '恋恋笔记本', '终结者2：审判日', '虎口脱险', '疯狂的麦克斯4：狂暴之路', '芙蓉镇', '罗生门', '新龙门客栈', '无耻混蛋', '崖上的波妞', '千钧一发', '花束般的恋爱', '彗星来的那一夜', '萤火虫之墓', '哈利·波特与死亡圣器(上)', '奇迹男孩', '黑客帝国2：重装上阵', '二十二', '火星救援', '白日梦想家', '高山下的花环', '战争之王', '血钻', '步履不停', '房间', '千年女优', '魂断蓝桥', '哈利·波特与凤凰社', '谍影重重2', '蜘蛛侠：平行宇宙', '大红灯笼高高挂', '弱点', '谍影重重', '阿飞正传', '朗读者', '再次出发之纽约遇见你', '燃情岁月']\n",
      "['9.7', '9.6', '9.5', '9.5', '9.4', '9.4', '9.5', '9.4', '9.4', '9.5', '9.4', '9.4', '9.3', '9.2', '9.3', '9.3', '9.2', '9.3', '9.6', '9.2', '9.4', '9.3', '9.3', '9.2', '9.1', '9.3', '9.2', '9.1', '9.3', '9.2', '9.2', '9.3', '9.0', '9.3', '9.1', '9.3', '9.4', '9.1', '9.0', '9.1', '9.0', '9.1', '9.2', '9.3', '9.3', '9.1', '9.3', '9.0', '9.2', '9.1', '9.2', '8.9', '9.1', '9.4', '9.2', '9.1', '9.1', '9.3', '9.1', '9.2', '9.2', '9.0', '9.0', '9.1', '9.2', '9.2', '8.9', '9.1', '8.9', '9.1', '8.8', '9.0', '8.8', '9.1', '9.1', '9.2', '8.9', '8.8', '8.9', '8.9', '8.9', '9.0', '8.9', '8.9', '9.0', '8.9', '9.3', '9.3', '9.0', '8.8', '9.2', '8.9', '8.9', '8.9', '9.1', '8.8', '8.8', '9.1', '9.1', '8.7', '8.9', '8.9', '8.9', '8.8', '8.8', '8.9', '8.8', '8.7', '8.9', '8.9', '8.8', '8.7', '8.9', '8.9', '8.9', '8.9', '8.8', '8.9', '8.7', '9.1', '8.8', '9.1', '8.8', '8.7', '8.8', '9.0', '9.1', '8.8', '8.9', '8.8', '8.8', '8.9', '8.8', '9.0', '9.0', '8.9', '8.9', '8.7', '9.6', '8.9', '8.6', '8.8', '8.7', '8.7', '8.8', '9.2', '8.7', '9.0', '9.0', '8.8', '8.8', '8.6', '8.8', '8.8', '8.7', '9.0', '9.3', '8.8', '8.7', '9.0', '8.7', '8.8', '8.5', '8.7', '8.7', '8.7', '8.8', '8.7', '8.8', '8.6', '9.0', '8.7', '8.9', '8.9', '8.9', '8.8', '8.8', '8.9', '8.7', '9.5', '8.6', '8.7', '9.1', '8.9', '8.7', '8.7', '8.7', '9.2', '8.7', '8.8', '8.6', '8.7', '8.5', '8.9', '9.0', '8.6', '8.7', '8.7', '8.6', '8.7', '8.9', '8.9', '8.6', '9.3', '8.5', '8.5', '8.6', '8.9', '8.6', '8.7', '8.6', '8.5', '8.7', '8.4', '8.5', '8.8', '8.9', '8.7', '9.3', '8.8', '8.7', '8.7', '8.6', '8.8', '8.6', '8.6', '8.7', '8.5', '8.6', '8.7', '8.7', '8.5', '8.6', '9.5', '8.7', '8.7', '8.8', '8.8', '8.8', '8.8', '8.5', '8.7', '8.6', '8.8', '8.7', '8.6', '8.5', '8.6', '8.6', '8.7']\n"
     ]
    }
   ],
   "source": [
    "# the first 250 movies\n",
    "movie_name = []  # movie name\n",
    "movie_star = []  # movie star\n",
    "\n",
    "\n",
    "for page in range(1, 11):\n",
    "    current='?start='+str((page-1)*25)\n",
    "    url_pages = url+current  \n",
    "    response = requests.get(url_pages, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    \n",
    "    for movie in soup.select('.info .hd'):\n",
    "        name = movie.select_one('.title').get_text(strip=True)\n",
    "        movie_name.append(name)\n",
    "\n",
    "\n",
    "    for star in soup.select('.info .bd .star'):\n",
    "        rating = star.select_one('.rating_num').get_text(strip=True)\n",
    "        movie_star.append(rating)\n",
    "\n",
    "print(movie_name)\n",
    "print(movie_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) Collecting into a dataframe\n",
    "\n",
    "Create a dataframe `movie_df` and add the data from the lists above to it. \n",
    "- `movie_name` is the movie name. Set the column name as `movie name`\n",
    "- `movie_star` is the population estimate via star. Add it to the dataframe, and set the column name as `movie star`\n",
    "\n",
    "Make sure to check the head of your dataframe to see that everything looks right! ie: movie_df.head()\n",
    "\n",
    "Finally, you should save the DataFrame to a csv file under this folder`'./output'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  movie name movie star\n",
      "0     肖申克的救赎        9.7\n",
      "1       霸王别姬        9.6\n",
      "2       阿甘正传        9.5\n",
      "3      泰坦尼克号        9.5\n",
      "4       千与千寻        9.4\n"
     ]
    }
   ],
   "source": [
    "csv_name =  \"MovieDouban.csv\"\n",
    "csv_dir = \"./output\"\n",
    "\n",
    "import pandas as pd\n",
    "movie_df = pd.DataFrame({\n",
    "    \"movie name\": movie_name,\n",
    "    \"movie star\": movie_star\n",
    "})\n",
    "\n",
    "print(movie_df.head())\n",
    "movie_df.to_csv(\"./output/movie_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: JSON File Analysis\n",
    "\n",
    "After the initial phase of web scraping, we transition to analyzing pre-collected data, which is often stored in accessible and structured formats like JSON and CSV. This approach allows us to bypass the time-consuming process of data collection through web scraping for certain datasets that are already available, enabling us to dive directly into data analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "In the section, you will first be working with a file called 'anon_user_dat.json'. You can find the given data under the folder `'./data/data_identifying'`. This file contains information about some (fake) Tinder users. When creating an account, each Tinder user was asked to provide their first name, last name, work email (to verify the disclosed workplace), age, gender, phone # and zip code. Before releasing this data, a data scientist cleaned the data to protect the privacy of Tinder's users by removing the obvious personal identifiers: phone #, zip code, and IP address. However, the data scientist chose to keep each users' email addresses because when they visually skimmed a couple of the email addresses none of them seemed to have any of the users' actual names in them. This is where the data scientist made a huge mistake!\n",
    "\n",
    "Data Files:\n",
    "- anon_user_dat.json\n",
    "- employee_info.json\n",
    "\n",
    "\n",
    "We will take advantage of having the work email addresses by finding the employee information of different companies and matching that employee information with the information we have, in order to identify the names of the secret Tinder users!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) Load data from JSON file \n",
    "\n",
    "Load the `anon_user_dat.json` json file into a pandas dataframe. Call it `df_personal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age                        email  gender\n",
      "0   60  gshoreson0@seattletimes.com    Male\n",
      "1   47           eweaben1@salon.com  Female\n",
      "2   27      akillerby2@gravatar.com    Male\n",
      "3   46            gsainz3@zdnet.com    Male\n",
      "4   72     bdanilewicz4@4shared.com    Male\n"
     ]
    }
   ],
   "source": [
    "df_personal = pd.read_json('./data/data_identifying/anon_user_dat.json')\n",
    "\n",
    "print(df_personal.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df_personal, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Check the first 10 emails \n",
    "\n",
    "Save the first 10 emails to a Series, and call it `sample_emails`. \n",
    "You should then print out this Series. ( Use print() )\n",
    "\n",
    "The purpose of this is to get a sense of how these work emails are structured and how we could possibly extract where each anonymous user seems to work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    gshoreson0@seattletimes.com\n",
      "1             eweaben1@salon.com\n",
      "2        akillerby2@gravatar.com\n",
      "3              gsainz3@zdnet.com\n",
      "4       bdanilewicz4@4shared.com\n",
      "5      sdeerness5@wikispaces.com\n",
      "6         jstillwell6@ustream.tv\n",
      "7         mpriestland7@opera.com\n",
      "8       nerickssen8@hatena.ne.jp\n",
      "9             hparsell9@xing.com\n",
      "Name: email, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sample_emails = df_personal['email'].head(10)\n",
    "print(sample_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(sample_emails, pd.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) Extract the Company Name From the Email \n",
    "\n",
    "Create a function with the following specifications:\n",
    "- Function Name: extract_company\n",
    "- Purpose: to extract the company of the email (i.e., everything after the @ sign but before the first .)\n",
    "- Parameter(s): email (string)\n",
    "- Returns: The extracted part of the email (string)\n",
    "- Hint: This should take 1 line of code. Look into the find('') method. \n",
    "\n",
    "You can start with this outline:\n",
    "```python \n",
    "def extract_company(email):\n",
    "    return\n",
    "```\n",
    "\n",
    "Example Usage: \n",
    "- extract_company(\"larhe@uber.com\") should return \"uber\"\n",
    "- extract_company(“ds@cogs.edu”) should return “cogs”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company(email):\n",
    "    return email.split('@')[1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert extract_company(\"gshoreson0@seattletimes.com\") == \"seattletimes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little bit of basic sleuthing (aka googling) and web-scraping (aka selectively reading in html code) it turns out that you've been able to collect information about all the present employees/interns of the companies you are interested in. Specifically, on each company website, you have found the name, gender, and age of its employees. You have saved that info in employee_info.json and plan to see if, using this new information, you can match the Tinder accounts to actual names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d) Load in employee data \n",
    "\n",
    "Load the json file into a pandas dataframe. Call it `df_employee`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   company first_name last_name  gender  age\n",
      "0  123-reg  Inglebert  Falconer    Male   42\n",
      "1      163     Rafael  Bedenham    Male   14\n",
      "2      163     Lemuel      Lind    Male   31\n",
      "3      163      Penny   Pennone  Female   45\n",
      "4      163       Elva  Crighton  Female   52\n"
     ]
    }
   ],
   "source": [
    "df_employee = pd.read_json('./data/data_identifying/employee_info.json')\n",
    "print(df_employee.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df_employee, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e) Match the employee name with company, age, gender \n",
    "\n",
    "Create a function with the following specifications:\n",
    "- Function name: employee_matcher\n",
    "- Purpose: to match the employee name with the provided company, age, and gender\n",
    "- Parameter(s): company (string), age (int), gender (string)\n",
    "- Returns: The employee first_name and last_name like this: return first_name, last_name \n",
    "- Note: If there are multiple employees that fit the same description, first_name and last_name should return a list of all possible first names and last names i.e., ['Desmund', 'Kelby'], ['Shepley', 'Tichner']. Note that the names of the individuals that would produce this output are 'Desmund Shepley' and 'Kelby Tichner'.\n",
    "\n",
    "Hint:\n",
    "There are many different ways to code this. An inelegant solution is to loop through `df_employee` \n",
    "   and for each data item see if the company, age, and gender match\n",
    "   i.e., \n",
    "   ```python\n",
    "   for i in range(0, len(df_employee)):\n",
    "             if (company == df_employee.loc[i,'company']):\n",
    "   ```\n",
    "   \n",
    "However! The solution above is very inefficient and long, so you should try to look into this:\n",
    "Google the df.loc method: It extracts pieces of the dataframe\n",
    "   if it fulfills a certain condition.\n",
    "   i.e., \n",
    "   \n",
    "```python\n",
    "df_employee.loc[df_employee['company'] == company]\n",
    "```\n",
    "\n",
    "If you need to convert your pandas data series into a list, you can do ```list(result)``` where result is a pandas \"series\"\n",
    "\n",
    "You can start with this outline:\n",
    "```python\n",
    "def employee_matcher(company, age, gender):\n",
    "    return first_name, last_name\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def employee_matcher(company, age, gender):\n",
    "\n",
    "    matched_employees = df_employee.loc[(df_employee['company'] == company) & \n",
    "                                        (df_employee['age'] == age) & \n",
    "                                        (df_employee['gender'] == gender)]\n",
    " \n",
    "    first_names = matched_employees['first_name'].tolist()\n",
    "    last_names = matched_employees['last_name'].tolist()\n",
    "    \n",
    "    return first_names, last_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert employee_matcher(\"google\", 41, \"Male\") == (['Maxwell'], ['Jorio'])\n",
    "assert employee_matcher(\"salon\", 47, \"Female\") == (['Elenore'], ['Gravett'])\n",
    "assert employee_matcher(\"webmd\", 28, \"Nonbinary\") == (['Zaccaria'], ['Bartosiak'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2f) Extract all the private data \n",
    "\n",
    "- Create 2 empty lists called `first_names` and `last_names`\n",
    "- Loop through all the people we are trying to identify in df_personal\n",
    "- Call the `extract_company` function (i.e., `extract_company(df_personal.loc[i, 'email'])` )\n",
    "- Call the `employee_matcher` function \n",
    "- Append the results of `employee_matcher` to the appropriate lists (`first_names` and `last_names`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_names=[]\n",
    "last_names=[]\n",
    "for i in range(len(df_personal)):\n",
    "   \n",
    "    company_name = extract_company(df_personal.loc[i, 'email'])\n",
    "  \n",
    "    age = df_personal.loc[i, 'age']\n",
    "    gender = df_personal.loc[i, 'gender']\n",
    "    \n",
    "    first_name, last_name = employee_matcher(company_name, age, gender)\n",
    "   \n",
    "    first_names.append(first_name)\n",
    "    last_names.append(last_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert first_names[45:50]== [['Justino'], ['Tadio'], ['Kennith'], ['Cedric'], ['Amargo']]\n",
    "assert last_names[45:50] == [['Corro'], ['Blackford'], ['Milton'], ['Yggo'], ['Grigor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2g) Add the names to the original 'secure' dataset! \n",
    "\n",
    "We have done this last step for you below, all you need to do is run this cell.\n",
    "\n",
    "For your own personal enjoyment, you should also print out the new `df_personal` with the identified people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personal['first_name'] = first_names\n",
    "df_personal['last_name'] = last_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Working with CSV Files\n",
    "Continuing with our exploration of pre-collected data formats, we delve into CSV files, which are renowned for their simplicity and widespread use in representing tabular data. This stage involves leveraging libraries like pandas in Python, which simplify the process of reading, manipulating, and analyzing CSV data. \n",
    "\n",
    "### Overview\n",
    "\n",
    "For this assignment, you are provided with two data files that contain information on a sample of people. The two files and their columns are:\n",
    "\n",
    "- `age_steps.csv`: Contains one row for each person.\n",
    "    - `id`: Unique identifier for the person.\n",
    "    - `age`: Age of the person.\n",
    "    - `steps`: Number of steps the person took on average in January 2018.\n",
    "    \n",
    "    \n",
    "- `incomes.json`: Contains one record for each person.\n",
    "    - `id`: Unique identifier for the person. Two records with the same ID between `age_steps.csv` and `incomes.json` correspond to the same person.\n",
    "    - `last_name`: Last name of the person.\n",
    "    - `first_name`: First name of the person.\n",
    "    - `income`: Income of the person in 2018.\n",
    "    \n",
    "You can find the given data under the folder `'./data/data_wrangling'`. To finish the assignment, we recommend looking at the official 10 minutes to pandas guide: http://pandas.pydata.org/pandas-docs/stable/10min.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3a:** Load the `age_steps.csv` file into a `pandas` DataFrame named `df_steps`. It should have 11257 rows and 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11257, 3)\n"
     ]
    }
   ],
   "source": [
    "df_steps=pd.read_csv('./data/data_wrangling/age_steps.csv')\n",
    "print(df_steps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df_steps, pd.DataFrame)\n",
    "assert df_steps.shape == (11257, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3b:** Load the `incomes.json` file into a `pandas` DataFrame called `df_income`. The DataFrame should have 13332 rows and 4 columns. \n",
    "\n",
    "Hint: Find a pandas function similar to `read_csv` for JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13332, 4)\n"
     ]
    }
   ],
   "source": [
    "df_income=pd.read_json('./data/data_wrangling/incomes.json')\n",
    "print(df_income.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df_income, pd.DataFrame)\n",
    "assert df_income.shape == (13332, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3c:** Drop the `first_name` and `last_name` columns from the `df_income` DataFrame. The resulting DataFrame should only have two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'income'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_income=df_income.drop(['first_name','last_name'],axis=1)\n",
    "print(df_income.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'first_name' not in df_income.columns\n",
    "assert 'last_name' not in df_income.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3d:** Merge the `df_steps` and `df_income` DataFrames into a single combined DataFrame called `df`. Use the `id` column to match rows together.\n",
    "\n",
    "The final DataFrame should have 10,135 rows and 4 columns: `id`, `income`, `age`, and `steps`.\n",
    "\n",
    "Call an appropriate `pandas` method to perform this operation; don't write a `for` loop. (In general, writing a `for` loop for a DataFrame will produce poor results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  age  steps    income\n",
      "0  36859   48   6764  10056.43\n",
      "1  99794   39   4308  13869.47\n",
      "2  33364   36   6410  79634.92\n",
      "3  73874   35   7870  12369.03\n",
      "4  66956   56   7670  41150.18\n"
     ]
    }
   ],
   "source": [
    "df=pd.merge(df_steps,df_income,on='id')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(df, pd.DataFrame)\n",
    "assert set(df.columns) == set(['id', 'income', 'age', 'steps'])\n",
    "assert df.shape == (10135, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3e:** Reorder the columns of `df` so that they appear in the order: `id`, `age`, `steps`, then `income`.\n",
    "\n",
    "(Note: If your DataFrame is already in this order, just put `df` in this cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  age  steps    income\n",
      "0  36859   48   6764  10056.43\n",
      "1  99794   39   4308  13869.47\n",
      "2  33364   36   6410  79634.92\n",
      "3  73874   35   7870  12369.03\n",
      "4  66956   56   7670  41150.18\n"
     ]
    }
   ],
   "source": [
    "#df = df[['id', 'age', 'steps', 'income']]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(df.columns) == ['id', 'age', 'steps', 'income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3f:** You may have noticed something strange: the merged `df` DataFrame has fewer rows than either of `df_steps` and `df_income`. Why did this happen? (If you're unsure, check out the documentation for the `pandas` method you used to merge these two datasets. Take note of the default values set for this method's parameters.)\n",
    "\n",
    "Please select the **one** correct explanation below and save your answer in the variable `q1f_answer`. For example, if you believe choice number 4 explains why `df` has fewer rows, set `q1f_answer = 4`.\n",
    "\n",
    "1. Some steps were recorded inaccurately in `df_steps`.\n",
    "2. Some incomes were recorded inaccurately in `df_income`.\n",
    "3. There are fewer rows in `df_steps` than in `df_income`.\n",
    "4. There are fewer columns in `df_steps` than in `df_income`.\n",
    "5. Some `id` values in either `df_steps` and `df_income` were missing in the other DataFrame.\n",
    "6. Some `id` values were repeated in `df_steps` and in `df_income`.\n",
    "\n",
    "You may use the cell below to run whatever code you want to check the statements above. Just make sure to set `q1f_answer` once you've selected a choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1f_answer=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(q1f_answer, int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Data Cleaning\n",
    "\n",
    "Post data collection, a pivotal step ensues—Data Cleaning. This phase is crucial for ensuring the reliability and accuracy of our analysis. It involves scrutinizing the data for inaccuracies, inconsistencies, and incompleteness. Techniques such as removing duplicates, handling missing values, and correcting errors are employed to refine the dataset. A common phenomenon is that the collected data may contain missing values. Here are two common ones:\n",
    "\n",
    "- **Nonresponse.** For example, people might have left a field blank when responding to a survey, or left the entire survey blank.\n",
    "- **Lost in entry.** Data might have been lost after initial recording. For example, a disk cleanup might accidentally wipe older entries of a database.\n",
    "\n",
    "In general, it is **not** appropriate to simply drop missing values from the dataset or pretend that if filled in they would not change your results. In 2016, many polls mistakenly predicted that Hillary Clinton would easily win the Presidential election by committing this error. In this particular dataset, however, the **missing values occur completely at random**. This criteria allows us to drop missing values without significantly affecting our conclusions.\n",
    "\n",
    "In this section, we continue use the data mentioned in Part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4a:** How many values are missing in the `income` column of `df`? Save this number into a variable called `n_nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451\n"
     ]
    }
   ],
   "source": [
    "n_nan = df['income'].isna().sum()\n",
    "print(n_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(n_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4b:** Remove all rows from `df` that have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9684, 4)\n"
     ]
    }
   ],
   "source": [
    "# Remove all rows from df that have missing data. In other words, remove all rows with NaN values.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "df=df.dropna()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(np.isnan(df['income'])) == 0\n",
    "assert df.shape == (9684, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4c:** Note that we can now compute the average income. If your `df` variable contains the right values, `df['income'].mean()` should produce the value `25508.84`.\n",
    "\n",
    "Suppose that we didn't drop the missing incomes. What will running `df['income'].mean()` output? Use the variable `q2c_answer` to record which of the below statements you think is true. As usual, you can use the cell below to run any code you'd like in order to help you answer this question as long as you set `q2c_answer` once you've finished.\n",
    "\n",
    "1. No change; `df['income'].mean()` will ignore the missing values and output `25508.84`.\n",
    "2. `df['income'].mean()` will produce an error.\n",
    "3. `df['income'].mean()` will output `0`.\n",
    "4. `df['income'].mean()` will output `nan` (not a number).\n",
    "5. `df['income'].mean()` will fill in the missing values with the average income, then compute the average.\n",
    "6. `df['income'].mean()` will fill in the missing values with `0`, then compute the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2c_answer=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(q2c_answer, int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4d:** Suppose that missing incomes did not occur at random, and that individuals with incomes below \\$10000 a year are less likely to report their incomes. If so, which of the following statements below is true? Record your choice in the variable `q2d_answer`.\n",
    "\n",
    "1. `df['income'].mean()` will likely output a value that is the same as the population's average income\n",
    "2. `df['income'].mean()` will likely output a value that is smaller than the population's average income.\n",
    "3. `df['income'].mean()` will likely output a value that is larger than the population's average income.\n",
    "4. `df['income'].mean()` will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2d_answer=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(q2d_answer, int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete!\n",
    "\n",
    "Congrats, you're done!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
